{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefd3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94858708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e568259",
   "metadata": {},
   "source": [
    "# Generate Kuzushiji images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9c572c",
   "metadata": {},
   "source": [
    "### Create z-vector to sample digit from latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e71298",
   "metadata": {},
   "source": [
    "First, we need to define the region of latent space in which we are sampling images for our digit. The construct_numvec function creates the z-vector that specifies this region for either the latent space for KMNIST or the latent space for K49. Additionally, it appends a one-hot encoding of the class label of the kuzushiji character we want to generate. For example, if we have a z-vector [0, 1, 1, 1, 0, 1, 0, 1, 0, 0] and we want to generate the third character from the KMNIST data set, the one-hot encoding of the third character would be [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], and that would be concatenated to the z-vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2734356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_numvec(digit: int, latent_dim: int=10, classes: int=10, z: list=None) -> list:\n",
    "    \"\"\"\n",
    "        digit: int -> the class of the kuzushiji character to be generated\n",
    "        latent_dim: int -> the number of dimensions in the latent space: equal to the number of classes\n",
    "        classes: int -> the number of class labels for a given model (10 for KMNIST, 49 for K49)\n",
    "        z: list -> the z-vector indicating the position in latent space to sample an character\n",
    "    \"\"\"\n",
    "    \n",
    "    out = np.zeros((1, latent_dim + classes))\n",
    "    out[:, digit + latent_dim] = 1.\n",
    "    if z is None:\n",
    "        return(out)\n",
    "    else:\n",
    "        for i in range(len(z)):\n",
    "            out[:,i] = z[i]\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4883ae86",
   "metadata": {},
   "source": [
    "Next, we need to define a function that generates images for a specified digit. This function will receive arguments to define the number of rows and cols in the latent space we want sample from. Essentially, the number of images generated will be rows * cols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cf9baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(digit: int=0, rows: int=10, cols: int=10, max_z: float=1.5, decoder=0, save_fig: bool=False) -> np.ndarray:\n",
    "    \"\"\" Return an array\n",
    "    \n",
    "            digit: int -> the class of the kuzushiji character to be generated\n",
    "            rows: int -> the width of the latent space\n",
    "            cols: int -> the height of the latent space\n",
    "            max_z: float ->\n",
    "            decoder: int -> the generator to be used to create augmented data (0 for K49, 1 for KMNIST)\n",
    "            save_fig: bool -> determines if an image of the augmented data should be created\n",
    "    \"\"\"\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    if decoder:\n",
    "        decoder = keras.models.load_model(\"decoder-KMNIST\")\n",
    "        latent_dim = 10\n",
    "        classes = 10\n",
    "    else:\n",
    "        decoder = keras.models.load_model(\"decoder-K49\")\n",
    "        latent_dim = 49\n",
    "        classes = 49\n",
    "\n",
    "    if save_fig:\n",
    "        fig, axs = plt.subplots(rows, cols)\n",
    "        fig.set_figwidth(28)\n",
    "        fig.set_figheight(28)\n",
    "\n",
    "    for i in range(0, rows):\n",
    "        z1 = (((i / (rows-1)) * max_z)*2) - max_z\n",
    "        for j in range(0, cols):\n",
    "            z2 = (((j / (cols-1)) * max_z)*2) - max_z\n",
    "            z_ = [z1, z2]\n",
    "            vec = construct_numvec(digit, latent_dim, classes, z_)\n",
    "            decoded = decoder.predict(vec)\n",
    "            images.append(decoded.reshape(28,28))\n",
    "            if save_fig:\n",
    "                axs[i, j].imshow(decoded.reshape(28,28), cmap=plt.cm.gray)\n",
    "                axs[i, j].axis('off')\n",
    "    \n",
    "    if save_fig:\n",
    "        fig.savefig(\"image_%d_%d_%d.png\" % (digit, rows, cols))\n",
    "        \n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890bf8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Generate some example images of the first class for KMNIST\n",
    "aug_data = generate_images(digit=0, rows=10, cols=10, max_z=1.5, decoder=1, save_fig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe8c17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb04abd",
   "metadata": {},
   "source": [
    "### Generate images from latent space for KMNIST or K49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33aaf5f",
   "metadata": {},
   "source": [
    "Let's generate some augmented data to train with. We'll start off with creating 100 sample images from each class of KMNIST. Feel free to change the number of images generated by altering the number of rows and columns in the generate_aug_kmnist() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c05ab9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_aug_kmnist(rows=10, cols=10):\n",
    "    images = generate_images(digit=0, rows=rows, cols=cols, max_z=1.5, decoder=1)\n",
    "    labels = []\n",
    "    labels.append([0 for x in range(rows) for y in range(cols)])\n",
    "    for digit in range(1,10):\n",
    "        aug_data = generate_images(digit=digit, rows=rows, cols=cols, max_z=1.5, decoder=1)\n",
    "        images = np.concatenate((images, aug_data), axis=0)\n",
    "        labels.append([digit for x in range(rows) for y in range(cols)])\n",
    "        \n",
    "    return images, np.array(labels).flatten()\n",
    "\n",
    "    \n",
    "def generate_aug_k49(rows=10, cols=10):\n",
    "    images = generate_images(digit=0, rows=rows, cols=cols, max_z=1.5, decoder=0)\n",
    "    labels = []\n",
    "    labels.append([0 for x in range(rows) for y in range(cols)])\n",
    "    for digit in range(1,49):\n",
    "        aug_data = generate_images(digit=digit, rows=rows, cols=cols, max_z=1.5, decoder=0)\n",
    "        images = np.concatenate((images, aug_data), axis=0)\n",
    "        labels.append([digit for x in range(cols) for y in range(cols)])\n",
    "        \n",
    "    return images, np.array(labels).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435e1182",
   "metadata": {},
   "source": [
    "When training the model later, make sure that if you desire to train using the K49 data set, comment out the line for generating KMNIST and uncomment for generating K49. The same applies for the KMNIST data. NOTE: it will take a few minutes to generate the augmented data for the K49 data set due to the large number of class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a80f14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "aug_kmnist_data, aug_kmnist_labels = generate_aug_kmnist()\n",
    "# aug_kmnist_data, aug_kmnist_labels = generate_aug_k49()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba17b4",
   "metadata": {},
   "source": [
    "### Define CNN model and set up training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb89b914",
   "metadata": {},
   "source": [
    "Let's define the CNN model we'll use to evaluate the quality of the augmented data. The functions below define the CNN model, the training process, and evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61744442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cnn model\n",
    "def define_model(kmnist=True):\n",
    "    \"\"\"\n",
    "        kmnist: bool -> specifies whether the CNN model to be constructed should be for KMNIST(True) or K49(False)\n",
    "    \"\"\"\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    if kmnist:\n",
    "        model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "        model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "        model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "        \n",
    "    else:\n",
    "        model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "        model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "        model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(keras.layers.Dense(49, activation='softmax'))\n",
    "        \n",
    "    # compile model\n",
    "    opt = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    " \n",
    "# evaluate a model using k-fold cross-validation\n",
    "def evaluate_model(dataX, dataY, X_test, epochs=10, n_folds=1, kmnist=True):\n",
    "    scores, histories, predictions = list(), list(), list()\n",
    "    callback = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=5)\n",
    "    \n",
    "    # If n_folds is less than 2, do not perform cross-validation\n",
    "    # Otherwise, implement cross-validation\n",
    "    if n_folds > 1:\n",
    "        # prepare cross validation\n",
    "        kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "        i = 1\n",
    "        for train_ix, test_ix in kfold.split(dataX):\n",
    "            print(len(train_ix), len(test_ix))\n",
    "            print(\"- - - - -\\nFold %d\\n- - - - -\" % i)\n",
    "            # define model\n",
    "            model = define_model(kmnist)\n",
    "            # select rows for train and test\n",
    "            trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "\n",
    "            # fit model\n",
    "            history = model.fit(trainX, trainY, epochs=epochs, batch_size=128, validation_data=(testX, testY), callbacks=[callback], verbose=1)\n",
    "            # evaluate model\n",
    "            _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "            print('> %.3f\\n' % (acc * 100.0))\n",
    "            prediction = model.predict(X_test)\n",
    "\n",
    "            # Save validation loss, accuracies, and predictions\n",
    "            scores.append(acc)\n",
    "            histories.append(history)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return scores, histories, predictions\n",
    "    \n",
    "    # define model\n",
    "    model = define_model(kmnist)\n",
    "    # select rows for train and test\n",
    "    trainX, testX, trainY, testY = train_test_split(dataX, dataY, test_size=0.20, random_state=42)\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit(trainX, trainY, epochs=epochs, batch_size=128, validation_data=(testX, testY), callbacks=[callback], verbose=1)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print('> %.3f\\n' % (acc * 100.0))\n",
    "    prediction = model.predict(X_test)\n",
    "\n",
    "    # Save validation loss, accuracies, and predictions\n",
    "    scores.append(acc)\n",
    "    histories.append(history)\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    return scores, histories, predictions\n",
    "\n",
    "    \n",
    "# Display graph for validation loss\n",
    "def summarize_diagnostics(histories):\n",
    "    for i in range(len(histories)):\n",
    "        # plot loss\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.title('Cross Entropy Loss')\n",
    "        plt.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "        plt.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "        # plot accuracy\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.title('Classification Accuracy')\n",
    "        plt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "        plt.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Display boxplot for accuracies\n",
    "def summarize_performance(scores):\n",
    "    # print summary\n",
    "    print('Accuracy: mean=%.3f std=%.3f, n=%d' % (np.mean(scores)*100, np.std(scores)*100, len(scores)))\n",
    "    # box and whisker plots of results\n",
    "    plt.boxplot(scores)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Train and evaluate the model(s)\n",
    "def run_test_harness(trainX, trainY, testX, testY, epochs=10, n_folds=1, kmnist=True):\n",
    "    # evaluate model\n",
    "    scores, histories, predictions = evaluate_model(trainX, trainY, testX, epochs=epochs, n_folds=n_folds, kmnist=kmnist)\n",
    "    \n",
    "    if n_folds > 1:\n",
    "        # learning curves\n",
    "        summarize_diagnostics(histories)\n",
    "        # summarize estimated performance\n",
    "        summarize_performance(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc1aa4f",
   "metadata": {},
   "source": [
    "### Load Kuzushiji data set and pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d08f0",
   "metadata": {},
   "source": [
    "Before we train, we need to load our data set that will be used to train the CNN model. For now, we'll start with using KMNIST. When we load the KMNIST data set, we will normalize the original and augmented data and then concatenate them together to form the new training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77b0707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale pixels\n",
    "def prep_pixels(train, *args):\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = None\n",
    "    if len(args) == 1:\n",
    "        test_norm = args[0].astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    if len(args) == 1:\n",
    "        test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset(kmnist=True):\n",
    "    \n",
    "    if kmnist:\n",
    "        # Load the KMNIST data set\n",
    "        training_data_files = \"./dataset/kmnist-train-imgs.npz\"\n",
    "        testing_data_files = \"./dataset/kmnist-test-imgs.npz\"\n",
    "        training_label_files = \"./dataset/kmnist-train-labels.npz\"\n",
    "        testing_label_files = \"./dataset/kmnist-test-labels.npz\"\n",
    "        \n",
    "        (trainX, trainY) = np.load(training_data_files)[\"arr_0\"], np.load(training_label_files)[\"arr_0\"]\n",
    "        (testX, testY) = np.load(testing_data_files)[\"arr_0\"], np.load(testing_label_files)[\"arr_0\"]\n",
    "\n",
    "        # Reshape the images for training \n",
    "        trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "        testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "        \n",
    "        # Convert class labels into one-hot encoding vectors\n",
    "        trainY, testY = keras.utils.to_categorical(trainY, num_classes=10), keras.utils.to_categorical(testY, num_classes=10)\n",
    "        \n",
    "        # prepare pixel data\n",
    "        trainX, testX = prep_pixels(trainX, testX)\n",
    "        \n",
    "        # Concatenate original KMNIST data with augmented data\n",
    "        trainX = np.concatenate((trainX, aug_kmnist_data), axis=0)\n",
    "        trainY = np.concatenate((trainY, aug_kmnist_labels_1h), axis=0)\n",
    "\n",
    "        return trainX, trainY, testX, testY\n",
    "    \n",
    "    # Load the K49 data set\n",
    "    training_data_files = \"./dataset/k49-train-imgs.npz\"\n",
    "    testing_data_files = \"./dataset/k49-test-imgs.npz\"\n",
    "    training_label_files = \"./dataset/k49-train-labels.npz\"\n",
    "    testing_label_files = \"./dataset/k49-test-labels.npz\"\n",
    "    \n",
    "    (trainX, trainY) = np.load(training_data_files)[\"arr_0\"], np.load(training_label_files)[\"arr_0\"]\n",
    "    (testX, testY) = np.load(testing_data_files)[\"arr_0\"], np.load(testing_label_files)[\"arr_0\"]\n",
    "    \n",
    "    # Reshape the images for training \n",
    "    trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "    testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "\n",
    "    # Convert class labels into one-hot encoding vectors\n",
    "    trainY, testY = keras.utils.to_categorical(trainY, num_classes=49), keras.utils.to_categorical(testY, num_classes=49)\n",
    "    \n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    \n",
    "    # Concatenate original K49 data with augmented data\n",
    "    trainX = np.concatenate((trainX, aug_kmnist_data), axis=0)\n",
    "    trainY = np.concatenate((trainY, aug_kmnist_labels_1h), axis=0)\n",
    "    \n",
    "    return trainX, trainY, testX, testY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bf4b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_kmnist_data, _ = prep_pixels(aug_kmnist_data)\n",
    "aug_kmnist_data = aug_kmnist_data.reshape((aug_kmnist_data.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6ec9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = np.unique(aug_kmnist_labels)\n",
    "aug_kmnist_labels_1h = keras.utils.to_categorical(aug_kmnist_labels, num_classes=len(unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3628bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "trainX, trainY, testX, testY = load_dataset(kmnist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e46ea",
   "metadata": {},
   "source": [
    "### Run Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f65ef",
   "metadata": {},
   "source": [
    "Now it's time to run! Run the function below and observe the progress of the CNN models as they train with the augmented data. If you want to train with the K49 data set, make sure to switch the kmnist argument from True to False, and remember to make changes to the code above as well to load the K49 data set. This means changing the value of the kmnist argument from True to False in the load_dataset() function above as well as calling the generate_aug_k49() function instead of generate_aug_kmnist()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4fbca3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "382/382 [==============================] - 21s 54ms/step - loss: 0.4768 - accuracy: 0.8476 - val_loss: 0.2241 - val_accuracy: 0.9280\n",
      "Epoch 2/5\n",
      "382/382 [==============================] - 20s 52ms/step - loss: 0.1845 - accuracy: 0.9407 - val_loss: 0.1651 - val_accuracy: 0.9474\n",
      "Epoch 3/5\n",
      "382/382 [==============================] - 20s 53ms/step - loss: 0.1355 - accuracy: 0.9553 - val_loss: 0.1496 - val_accuracy: 0.9502\n",
      "Epoch 4/5\n",
      "382/382 [==============================] - 20s 53ms/step - loss: 0.1083 - accuracy: 0.9642 - val_loss: 0.1327 - val_accuracy: 0.9576\n",
      "Epoch 5/5\n",
      "382/382 [==============================] - 20s 53ms/step - loss: 0.0940 - accuracy: 0.9684 - val_loss: 0.1312 - val_accuracy: 0.9593\n",
      "> 95.926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_test_harness(trainX, trainY, testX, testY, epochs=5, n_folds=1, kmnist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06eee07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
